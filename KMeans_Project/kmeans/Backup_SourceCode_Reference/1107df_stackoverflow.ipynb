{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbf1d27",
   "metadata": {},
   "source": [
    "Source: \n",
    "https://stackoverflow.com/questions/47585723/kmeans-clustering-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8543c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\leoshr\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in c:\\users\\leoshr\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (5.8.0)\n",
      "sc:<SparkContext master=local appName=Kmeans>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# $example off$\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "from pyspark.context import SparkContext,SparkConf\n",
    "from pyspark.rdd import RDD\n",
    "!pip install psutil\n",
    "import psutil\n",
    "#若沒有下面這段\n",
    "#會有Exception: Java gateway process exited before sending its port number\n",
    "#Config\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jre1.8.0_301'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"Kmeans\").setMaster(\"local\"))\n",
    "print(f'sc:{sc}')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93c58d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12704e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\LeoShr\\\\p_space\\\\NTHU\\\\MDA\\\\CH3_K_means'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f0cab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '1106Trial_利用RDD.ipynb',\n",
       " '1106Trial_試圖用df做失敗.ipynb',\n",
       " '1107df_很好的stackoverflow文章 成功.ipynb',\n",
       " '1107df_很好的stackoverflow文章_模擬資料.ipynb',\n",
       " '1107Spark ml_df官方範例.ipynb',\n",
       " 'Apache_PySpark_Non-custom_Kmeans_1.ipynb',\n",
       " 'Apache_PySpark_Non-custom_Kmeans_2.ipynb',\n",
       " 'Credit Card Dataset for Clustering_K_means_成功.ipynb',\n",
       " 'dataset',\n",
       " 'K-means clustering_hacker_pyspark__mocking.ipynb',\n",
       " 'kmeans',\n",
       " 'KMeans function.ipynb',\n",
       " 'MDA_HW3.pdf',\n",
       " 'Project',\n",
       " 'target',\n",
       " 'testing5_.ipynb',\n",
       " 'Trail_3_testing.ipynb']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.chdir('C:\\\\Users\\\\LeoShr\\\\p_space\\\\NTHU\\\\MDA\\\\CH3_K_means') #change the current working directory to specified path. \n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683a2be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6f7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8ad1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[0, 33.3, -17.5],\n",
    "                              [1, 40.4, -20.5],\n",
    "                              [2, 28., -23.9],\n",
    "                              [3, 29.5, -19.0],\n",
    "                              [4, 32.8, -18.84]\n",
    "                             ],\n",
    "                              [\"other\",\"lat\", \"long\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd01a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[other: bigint, lat: double, long: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4345bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|other| lat|  long|\n",
      "+-----+----+------+\n",
      "|    0|33.3| -17.5|\n",
      "|    1|40.4| -20.5|\n",
      "|    2|28.0| -23.9|\n",
      "|    3|29.5| -19.0|\n",
      "|    4|32.8|-18.84|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf02583",
   "metadata": {},
   "source": [
    "### Step 1 - assemble your features\n",
    "\n",
    "In contrast to most ML packages out there, Spark ML requires your input features to be gathered in a single column of your dataframe, usually named features; and it provides a specific method for doing this, VectorAssembler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e16671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+-------------+\n",
      "|other| lat|  long|     features|\n",
      "+-----+----+------+-------------+\n",
      "|    0|33.3| -17.5| [33.3,-17.5]|\n",
      "|    1|40.4| -20.5| [40.4,-20.5]|\n",
      "|    2|28.0| -23.9| [28.0,-23.9]|\n",
      "|    3|29.5| -19.0| [29.5,-19.0]|\n",
      "|    4|32.8|-18.84|[32.8,-18.84]|\n",
      "+-----+----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"lat\", \"long\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.show()\n",
    "\n",
    "# As perhaps already guessed, the argument inputCols serves to tell VectoeAssembler which particular columns \n",
    "# in our dataframe are to be used as features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc467a8",
   "metadata": {},
   "source": [
    "## Step 2 - fit your KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3acbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=2, seed=1)  # 2 clusters here\n",
    "model = kmeans.fit(new_df.select('features'))\n",
    "\n",
    "# select('features') here serves to tell the algorithm \n",
    "# which column of the dataframe to use for clustering - remember that, after Step 1 above, your original lat & long features \n",
    "# are no more directly used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582f3d2",
   "metadata": {},
   "source": [
    "### Step 3 - transform your initial dataframe to include cluster assignments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb57a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+-------------+----------+\n",
      "|other| lat|  long|     features|prediction|\n",
      "+-----+----+------+-------------+----------+\n",
      "|    0|33.3| -17.5| [33.3,-17.5]|         0|\n",
      "|    1|40.4| -20.5| [40.4,-20.5]|         0|\n",
      "|    2|28.0| -23.9| [28.0,-23.9]|         1|\n",
      "|    3|29.5| -19.0| [29.5,-19.0]|         1|\n",
      "|    4|32.8|-18.84|[32.8,-18.84]|         0|\n",
      "+-----+----+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(new_df)\n",
    "transformed.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4feed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last column of the transformed dataframe, prediction, shows the cluster assignment - in my toy case, I have ended up with 4 records in cluster #0 and 1 record in cluster #1.\n",
    "\n",
    "# You can further manipulate the transformed dataframe with select statements, or even drop the features column (which has now fulfilled its function and may be no longer necessary)...\n",
    "\n",
    "# Hopefully you are much closer now to what you actually wanted to achieve in the first place. For extracting cluster statistics etc., another recent answer of mine might be helpful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ffe8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#延伸: https://stackoverflow.com/questions/47130299/pyspark-ml-get-kmeans-cluster-statistics/47156822#47156822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac9a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e392fd44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21608/3278402326.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import dataset\n",
    "load(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8860a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('iris.csv')\n",
    "data.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
