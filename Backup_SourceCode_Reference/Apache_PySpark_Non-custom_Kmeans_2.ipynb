{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1e1eaa",
   "metadata": {},
   "source": [
    "## Simulating the tutorial of Official Tutorial\n",
    "(non-costom distance function)\n",
    "\n",
    "Tutorial: \n",
    "\n",
    "http://spark.apache.org/docs/latest/mllib-clustering.html\n",
    "\n",
    "\n",
    "data:\n",
    "https://github.com/apache/spark/tree/master/data/mllib\n",
    "\n",
    "\n",
    "In general using a different distance measure doesn't make sense, because k-means (unlike k-medoids) algorithm is well defined only for Euclidean distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d94f58",
   "metadata": {},
   "source": [
    "### Why does k-means clustering algorithm use only Euclidean distance metric?\n",
    "\n",
    "The term \"centroid\" is itself from Euclidean geometry. It is multivariate mean in euclidean space. Euclidean space is about euclidean distances. Non-Euclidean distances will generally not span Euclidean space. That's why K-Means is for Euclidean distances only.\n",
    "\n",
    "\n",
    "Source: https://stats.stackexchange.com/questions/81481/why-does-k-means-clustering-algorithm-use-only-euclidean-distance-metric\n",
    "\n",
    "If you are looking for an Manhattan-distance variant of k-means, there is k-medians. Because the median is a known best L1 estimator.\n",
    "\n",
    "If you want arbitrary distance functions, have a look at k-medoids (aka: PAM, partitioning around medoids). The medoid minimizes arbitrary distances (because it is defined as the minimum), and there only exist a finite number of possible medoids, too. It is much more expensive than the mean, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae0a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grant the total CPU info\n",
    "# !pip install psutil\n",
    "# import psutil,datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24f4afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc:<SparkContext master=local appName=Kmeans>\n"
     ]
    }
   ],
   "source": [
    "#api for k-means algo\n",
    "from pyspark.ml.clustering import KMeans,KMeansModel\n",
    "\n",
    "#ClusteringEvaluator\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.ClusteringEvaluator.html\n",
    "#The metric computes the Silhouette measure using the squared Euclidean distance.\n",
    "\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "#Build the api for the entrance\n",
    "from pyspark.context import SparkContext,SparkConf\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "#Config\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jre1.8.0_301'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"Kmeans\").setMaster(\"local\"))\n",
    "print(f'sc:{sc}')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d08bb",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "559aa246",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'getpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7740/3971706588.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'os' has no attribute 'getpath'"
     ]
    }
   ],
   "source": [
    "os.getpath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "191fa1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data\n",
    "path = r'C:\\Users\\LeoShr\\p_space\\NTHU\\MDA\\CH3_K_means\\dataset\\sample_kmeans_data.txt'\n",
    "#dataset = spark.read.format(\"libsvm\").load(path) #this is used to import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9065b8d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'maxIterations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7740/2405533899.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparsedData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsedData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializationMode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"random\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'maxIterations'"
     ]
    }
   ],
   "source": [
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "clusters = KMeans.fit(parsedData, 2, maxIterations=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "896daee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[28] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]#list\n",
    "rdd = spark.sparkContext.parallelize(dept)#list 2 RDD\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d36dbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeoShr\\p_space\\NTHU\\MDA\\CH3_K_means\\dataset\\sample_kmeans_data.txt MapPartitionsRDD[43] at textFile at <unknown>:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0 1:0.0 2:0.0 3:0.0',\n",
       " '1 1:0.1 2:0.1 3:0.1',\n",
       " '2 1:0.2 2:0.2 3:0.2',\n",
       " '3 1:9.0 2:9.0 3:9.0',\n",
       " '4 1:9.1 2:9.1 3:9.1',\n",
       " '5 1:9.2 2:9.2 3:9.2']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load 成rdd\n",
    "data = sc.textFile(path)\n",
    "print(data)\n",
    "data.collect()\n",
    "\n",
    "# #PySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe\n",
    "# #load 成df\n",
    "# df_data = data.toDF()\n",
    "# df_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5aa761c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1:0.0', '2:0.0', '3:0.0'],\n",
       " ['1', '1:0.1', '2:0.1', '3:0.1'],\n",
       " ['2', '1:0.2', '2:0.2', '3:0.2'],\n",
       " ['3', '1:9.0', '2:9.0', '3:9.0'],\n",
       " ['4', '1:9.1', '2:9.1', '3:9.1'],\n",
       " ['5', '1:9.2', '2:9.2', '3:9.2']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not working\n",
    "#parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "parsedData = data.map(lambda x: x.split(' '))\n",
    "parsedData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2953c59",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2adf2c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7740/2931729827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d5d52da",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7740/1675300417.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Trains a k-means model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetK\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetSeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsedData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "# Build the model (cluster the data)\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(parsedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc891ae",
   "metadata": {},
   "source": [
    "\n",
    "### AttributeError: 'PipelinedRDD' object has no attribute '_jdf'\n",
    "\n",
    "MultilayerPerceptronClassifier is available with pyspark.ml and it works with DataFrame only while pyspark.mllib works with RDD and MultilayerPerceptronClassifier is not available under mlLib (and it will never be), now I have to change the way I load the data in Spark ans load it as a dataframe \n",
    "\n",
    "Stackoverflow: https://stackoverflow.com/questions/39643185/pipelinedrdd-object-has-no-attribute-jdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36bdda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the model (cluster the data)\n",
    "# clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "maxIterations=10\n",
    "kmeans = KMeans(k = maxIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fd8ad28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans_628ea7e40eaf"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c94ab873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans_628ea7e40eaf"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.setWeightCol(\"weighCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cdccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fda10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046eab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05238a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9bf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
